---
title: "Discussion"
---

# 5 Discussion and Deeper Insights

> *Note – This chapter digs beyond headline metrics. We reflect on what the numbers really tell us and when you might reach for **LIME** or **SHAP** in day‑to‑day practice. The goal is practical wisdom, not just another comparison table.*

------------------------------------------------------------------------

## 5.1 What the Experiments Actually Showed

| Model | Accuracy | AUC | Balanced Accuracy | Where It Shines |
|--------------|--------------|--------------|--------------|-----------------|
| **Random Forest** | 0.74 | 0.884 | 0.79 | Built‑in feature importance, quick to train, naturally robust to class imbalance |
| **Neural Network (1‑hidden‑layer MLP)** | **0.76** | **0.899** | **0.81** | Flexible with non‑linear patterns, easy to scale deeper if needed |

Small caveat: the MLP wins on AUC by a hair, but confusion‑matrix results tell us that neither model dominates across every metric. In short, throwing a deeper network at this tabular problem does **not** magically beat well‑tuned trees.

------------------------------------------------------------------------

## 5.2 LIME vs. SHAP — Theory Meets Reality

|  Dimension  | **LIME** | **SHAP** |
|------------------|-------------------------|-----------------------------|
| **Core Idea** | Perturb the neighbourhood of one sample, fit a tiny linear model to mimic the big model locally | Shapley game‑theory decomposition: prediction = baseline + fair share from each feature |
| **Local / Global** | Purely local (one explanation ≈ one sample) | Local **and** global (aggregate Shapley values) |
| **Compute Cost** | O( #perturbations × #features ); high‑dimensional data ↗ runtime | Theoretical O(2\^k), but tree‑/kernel shortcuts make single‑point SHAP run in seconds |
| **Where Stability Comes From** | Careful choice of neighbourhood size, sampling distribution, regularisation | Shapley axioms (consistency, local accuracy) guarantee a unique solution |
| **Sweet Spots** | 1\) Intuitive bar charts  2) Works with any black box  3) User can tweak sampling | 1\) Strong theory  2) Additive outputs = easy to sum, average, sort  3) Tree SHAP near‑linear time on forests |
| **Gotchas** | Sensitive to parameters; stability drops in very wide feature spaces | Deep / kernel SHAP on heavy networks is still pricey; outliers can stretch Shapley values |

------------------------------------------------------------------------

## 5.3 Does Model Complexity Change the Story?

### Simple models – shallow NNs, forests

-   Transparency is already decent (think Gini, raw weights).
-   LIME and SHAP mostly agree on the top handful of features (≈ 60 % overlap here).
-   Easy to explain to a product manager or policy analyst – nothing too exotic.

### Complex stacks – deep nets, boosted ensembles

-   Lots of gradient pathways and feature interactions; a single method rarely captures every nuance.
-   Kernel or Deep SHAP still works, but memory/time spikes; LIME needs far more permutations and a good distance metric.
-   Redundant features blow up the search space. Shapley’s additive fairness helps; LIME may wobble under multicollinearity.

**Take‑away:** the more tangled your model, the more you lean on SHAP for a principled global view. LIME remains the lightweight microscope when you only care about a handful of critical decisions.

------------------------------------------------------------------------

## 5.4 Reading the Extra Plots

-   **Correlation heatmap** – Key numeric features barely correlate with each other, so the model isn’t just riding a single linear signal; non‑linear effects matter.
-   **Parallel‑coords** – `capital_gain` shows a step‑function split: high earners bunch at discrete gain levels, while `fnlwgt` only flags income at the extremes.
-   **UMAP 2‑D embedding** – Red and blue points overlap heavily; even perfect feature engineering would struggle to push AUC beyond \~0.9 on this dataset.

------------------------------------------------------------------------

## 5.5 Which Tool When?

| Real‑world Need | Pick This | Why |
|----------------------|------------------|--------------------------------|
| **Real‑time decision service** | **Tree / Deep SHAP** | Millisecond latency per case, additive explanation satisfies auditors |
| **Offline batch audit or debugging** | **LIME + intrinsic importance** | Quickly surfaces weird edge cases and local boundaries; cross‑checks with RF importance |
| **You care about feature interactions or “what‑if” tweaks** | **SHAP interaction, counterfactuals** | Quantifies synergy terms, shows how to flip a decision |
| **Images, text, other unstructured inputs** | **Deep SHAP / Integrated Gradients** | LIME super‑pixels need heavy tuning; gradient‑based SHAP more stable |

------------------------------------------------------------------------

## 5.6 Limitations & What We’d Do Next

1.  **Correlation ≠ causation.** Both methods are descriptive, not causal. Next step: marry SHAP with DoWhy or causal SHAP.
2.  **Uncertainty.** We checked LIME stability but didn’t build CIs for Shapley values. Bootstrap‑SHAP could change that.
3.  **Visual analytics.** Embedding these explanations into a Shiny / Dash dashboard would let domain experts poke around without code.

------------------------------------------------------------------------

## 5.7 Final Thoughts

Relying on a **single** interpretability metric is a trap. The blend of SHAP’s global fairness and LIME’s local storytelling covers most practical needs:

-   **Regulators** trust SHAP’s additive, consistent attributions.
-   **Engineers and analysts** appreciate LIME’s quick, tweakable local views for debugging.

Mix them – plus interactive visuals – and you’re well on your way to genuinely transparent, trustworthy machine‑learning systems.
